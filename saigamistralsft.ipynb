{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7016635,"sourceType":"datasetVersion","datasetId":4006737,"isSourceIdPinned":true},{"sourceId":7580148,"sourceType":"datasetVersion","datasetId":4412505},{"sourceId":7580638,"sourceType":"datasetVersion","datasetId":4412807}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Дообучение Saiga-Mistral, квантизация и инференс с помощью llama-cpp","metadata":{}},{"cell_type":"markdown","source":"В репозитории реализован код для дообучения русскоязычной LLM [Saiga mistral](https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora), а также её квантизация и запуск с помощью llama-cpp. Попытался сделать код максимально гибким и воспроизводимым.  \nПредполагается запуск на GPU. Может запускаться на multi-gpu без доп. модификаций.  \nПри создании ноутбука опирался на эту [статью](https://habr.com/ru/articles/776872/) на Хабре, задекорировал и актуализировал некоторые моменты","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, Trainer, TrainingArguments, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\nfrom datasets import load_dataset\nimport transformers\nimport torch\nimport time\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:01:46.985637Z","iopub.execute_input":"2025-01-02T09:01:46.985943Z","iopub.status.idle":"2025-01-02T09:01:53.353255Z","shell.execute_reply.started":"2025-01-02T09:01:46.985918Z","shell.execute_reply":"2025-01-02T09:01:53.352352Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"IlyaGusev/saiga2_7b_lora\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nst_time = time.time()\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n\nconfig = PeftConfig.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    load_in_8bit = True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n\n)\nmodel = PeftModel.from_pretrained(\n    model,\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    is_trainable = True,\n    quantization=bnb_config\n)\n\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\n\nprint(generation_config)\nprint(f'Загрузка модели заняла {round(time.time() - st_time, 2)} секунд')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:01:58.901215Z","iopub.execute_input":"2025-01-02T09:01:58.902119Z","iopub.status.idle":"2025-01-02T09:02:06.638472Z","shell.execute_reply.started":"2025-01-02T09:01:58.902069Z","shell.execute_reply":"2025-01-02T09:02:06.637770Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66f8b1c6fdd43e88a5cdbc572b7b897"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"},{"name":"stdout","text":"GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_new_tokens\": 3584,\n  \"no_repeat_ngram_size\": 15,\n  \"pad_token_id\": 0,\n  \"repetition_penalty\": 1.2,\n  \"temperature\": 0.5,\n  \"top_k\": 30,\n  \"top_p\": 0.9\n}\n\nЗагрузка модели заняла 7.67 секунд\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:02:19.345757Z","iopub.execute_input":"2025-01-02T09:02:19.346073Z","iopub.status.idle":"2025-01-02T09:02:19.354695Z","shell.execute_reply.started":"2025-01-02T09:02:19.346047Z","shell.execute_reply":"2025-01-02T09:02:19.353695Z"}},"outputs":[{"name":"stdout","text":"trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:02:23.982272Z","iopub.execute_input":"2025-01-02T09:02:23.982612Z","iopub.status.idle":"2025-01-02T09:02:24.231526Z","shell.execute_reply.started":"2025-01-02T09:02:23.982580Z","shell.execute_reply":"2025-01-02T09:02:24.230723Z"}},"outputs":[{"name":"stdout","text":"Thu Jan  2 09:02:24 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   65C    P0             31W /   70W |    3195MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   71C    P0             33W /   70W |    3923MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"### Загрузка датасета","metadata":{}},{"cell_type":"markdown","source":"Датасет для дообучения должен быть в формате json и иметь формат ```[{\"system\": str, \"user\": str, \"bot\": str}, ... ]```, где system - системное сообщение для модели (например, у Сайги в use-example это \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"), user - это промпт пользователя, bot - ответ модели.","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input\"))\n\n\nfrom os import walk\nfilenames = next(walk(\"/kaggle/input/fine-tunning-llama/\"), (None, None, []))[2] \nfilenames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T09:08:49.400267Z","iopub.execute_input":"2025-01-02T09:08:49.400607Z","iopub.status.idle":"2025-01-02T09:08:49.408287Z","shell.execute_reply.started":"2025-01-02T09:08:49.400576Z","shell.execute_reply":"2025-01-02T09:08:49.407320Z"}},"outputs":[{"name":"stdout","text":"['fine-tunning-llama']\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['train.json', 'val.json']"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"TRAIN_PATH = \"train.json\"\nVALID_PATH = \"val.json\"\n\ndata = load_dataset(\n    \"json\", \n    data_files={\n                'train': TRAIN_PATH,\n                'validation': VALID_PATH\n    }\n)\ndata[\"train\"] = data[\"train\"].shuffle() # for train data shuffling, optional","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"## Предобработка датасета","metadata":{}},{"cell_type":"code","source":"CUTOFF_LEN = 2500 # до какого токена будет обрезать текст\n\n\ndef generate_prompt(data_point):\n    prompt = f\"\"\"<s>system\n{data_point['system']}</s><s>user\n{data_point['user']}</s><s>bot\n{data_point['bot']}[</s>\"\"\"\n    return prompt\n \n    \ndef tokenize(prompt, add_eos_token=True):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN,\n        padding=False,\n        return_tensors=None,\n    )\n    if (\n        result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < CUTOFF_LEN\n        and add_eos_token\n    ):\n        \n        result[\"input_ids\"].append(tokenizer.eos_token_id)\n        result[\"attention_mask\"].append(1)\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\n\ndef generate_and_tokenize_prompt(data_point):\n    full_prompt = generate_prompt(data_point)\n    tokenized_full_prompt = tokenize(full_prompt)\n    return tokenized_full_prompt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = (\n    data[\"train\"].map(generate_and_tokenize_prompt)\n)\n\nval_data = (\n    data[\"validation\"].map(generate_and_tokenize_prompt)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"## Обучение модели","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 6\nMICRO_BATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nLEARNING_RATE = 3e-4\nTRAIN_EPOCHS = 5\nOUTPUT_DIR = \"finetuned_model\"\n\ntraining_arguments = transformers.TrainingArguments(\n            per_device_train_batch_size=MICRO_BATCH_SIZE,\n            per_device_eval_batch_size=MICRO_BATCH_SIZE,\n            prediction_loss_only=True,\n            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n            num_train_epochs=TRAIN_EPOCHS,\n            learning_rate=LEARNING_RATE,\n            fp16=True,\n            logging_steps=25000,\n            optim=\"adamw_torch\",\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            output_dir=OUTPUT_DIR,\n            load_best_model_at_end=True,\n            report_to=None,\n            overwrite_output_dir=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = transformers.DataCollatorForSeq2Seq(\n    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=training_arguments,\n    data_collator=data_collator\n)\nmodel = torch.compile(model)\ntrainer.train()\nmodel.save_pretrained(OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"## Квантизация модели","metadata":{}},{"cell_type":"markdown","source":"Для начала склонируем репозитории с библиотеками rulm и llama-cpp для конкатенации обученного адаптера и квантизации.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/IlyaGusev/rulm.git\n!git clone https://github.com/ggerganov/llama.cpp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"### Склеим модель и обученный адаптер","metadata":{}},{"cell_type":"code","source":"from rulm.self_instruct.src.tools import convert_to_native","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH_TO_CHECKPOINT = \"finetuned_model/checkpoint-66\" # путь до чекпоинта адаптера, который хотим приклеить\nMERGED_MODEL_PATH = \"merged_model.pt\"\n\nconvert_to_native.convert_to_native(PATH_TO_CHECKPOINT, MERGED_MODEL_PATH, \n                                    device=\"cuda\", enable_offloading=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"### Конвертируем склеенную модель в 16-битный формат GGUF для запуска с помощью llama-cpp","metadata":{}},{"cell_type":"code","source":"# сперва сохраним токенайзер в папку, где лежит лучший чекпоинт\n\ntokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga2_7b_lora\", use_fast=False)\ntokenizer.save_pretrained(PATH_TO_CHECKPOINT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"Обязательная строчка, надо откатить версию llama-cpp, т.к. на последней квантизация почему-то не работает.","metadata":{}},{"cell_type":"code","source":"%cd llama.cpp\n!git checkout 64e64aa","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_PATH = \"../model-f16.gguf\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python convert.py {os.path.join(\"..\", MERGED_MODEL_PATH)} --vocab-dir {os.path.join(\"..\", PATH_TO_CHECKPOINT)} --outfile {OUTPUT_PATH} --outtype f16 --ctx 4096","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"### Квантуем моедль в 4 бита и 8 бит","metadata":{}},{"cell_type":"code","source":"!make quantize","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"QUANT_MODEL = \"../model-q4_0.gguf\"\nQUANTIZATION_TYPE = \"q4_0\" # \"q4_0\" или \"q4_1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! ./quantize {OUTPUT_PATH} {QUANT_MODEL} {QUANTIZATION_TYPE}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"markdown","source":"### Запуск скомпилированной версии на GPU с помощью llama-cpp","metadata":{}},{"cell_type":"markdown","source":"Переустановим llama-cpp на последнюю версию. Параметры, которые идут перед установкой, обязательны для запуска на GPU.","metadata":{}},{"cell_type":"code","source":"!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"### Использование модели в питоновском коде","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm = Llama(model_path=\"../model-q4_0.gguf\", n_gpu_layers=128, n_ctx=2048)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   ","metadata":{}},{"cell_type":"code","source":"prompt = f\"\"\"<s>system\n    {'Any system prompt'}</s><s>user\n    {'Any user prompt'}</s><s>bot\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\noutput = llm(\n      prompt, # Prompt\n      max_tokens=2048,\n      echo=False,\n      temperature=0\n)\n\nprint(time.time() - start_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(output[\"choices\"][0][\"text\"][:-1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}